{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State(s)\n",
    "     - A state s is a complete description of the state of the world. There is no information about the world which is hidden from the state. \n",
    "-  observation\n",
    "    - An observation o is a partial description of a state, which may omit information.\n",
    "- Policy\n",
    "     - A policy is a rule used by an agent to decide what actions to take\n",
    "          - Deterministic Policy\n",
    "               $$a_t = \\mu(s_t)$$\n",
    "          - Stochastic Policy\n",
    "               $$a_t \\sim \\pi(\\cdot | s_t)$$\n",
    "     - Parameterized policies\n",
    "          - policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm.\n",
    "          - We often denote the parameters of such a policy by \\theta or \\phi, and then write this as a subscript on the policy symbol to highlight the connection:\n",
    "               \n",
    "          $$a_t = \\mu_{\\theta}(s_t)$$\n",
    "          $$a_t \\sim \\pi_{\\theta}(\\cdot | s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic Policies\n",
    "    - stochastic policies in deep RL are categorical policies and diagonal Gaussian policies.\n",
    "    - `Categorical policies` can be used in discrete action spaces\n",
    "    - `diagonal Gaussian` policies are used in continuous action spaces.\n",
    "\n",
    "- Two key computations are centrally important for using and training `stochastic policies`:\n",
    "\n",
    "    - sampling actions from the policy,\n",
    "    - computing log likelihoods of particular actions\n",
    "         $$\\log \\pi_{\\theta}(a|s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Trajectory\n",
    "    - A trajectory \\tau is a sequence of states and actions in the world, Trajectories are also frequently called `episodes`\n",
    "\n",
    "    $$\\tau = (s_0, a_0, s_1, a_1, ...)$$\n",
    "\n",
    "    - The very first state of the world, s_0, is randomly sampled from the start-state distribution, sometimes denoted by \\rho_0:\n",
    "\n",
    "        $$s_0 \\sim \\rho_0(\\cdot)$$\n",
    "\n",
    "    - State transitions (what happens to the world between the state at time t, s_t, and the state at t+1, s_{t+1}), are governed by the natural laws of the environment, and depend on only the most recent action, a_t. They can be either `deterministic`,\n",
    "\n",
    "        $$s_{t+1} = f(s_t, a_t)$$\n",
    "\n",
    "    - or `stochastic`,\n",
    "\n",
    "        $$s_{t+1} \\sim P(\\cdot|s_t, a_t)$$\n",
    "\n",
    "    - In the context of the expression the dot (⋅) represents a placeholder for a variable. In probability notation, the dot is often used to represent the variable of interest or the outcome of an event in a random variable context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reward and Return\n",
    "    - The reward function `R` is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world:\n",
    "\n",
    "    $$r_t = R(s_t, a_t, s_{t+1})$$\n",
    "\n",
    "    - although frequently this is simplified to just a dependence on the \n",
    "    - `current state`, \n",
    "    $$r_t = R(s_t)$$\n",
    "\n",
    "    - `state-action pair` \n",
    "        $$r_t = R(s_t,a_t)$$\n",
    "        \n",
    "    \n",
    "    - One kind of return is the `finite-horizon undiscounted return`, which is just the sum of rewards obtained in a fixed window of steps:\n",
    "\n",
    "        $$R(\\tau) = \\sum_{t=0}^T r_t$$\n",
    "\n",
    "        - In a finite-horizon setting, the agent operates for a fixed and known number of time steps.\n",
    "        - There's a clear \"end point\" to the decision-making process.\n",
    "        - The value function in this case needs to consider the specific time step because the agent's goal is to maximize the total undiscounted reward accumulated within that finite horizon.\n",
    "\n",
    "    - Another kind of return is the `infinite-horizon discounted return`, which is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained. This formulation of reward includes a discount factor \\gamma \\in (0,1):\n",
    "\n",
    "        $$R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t$$\n",
    "\n",
    "    - In an infinite-horizon discounted setting, the agent is assumed to operate for an indefinite period.\n",
    "    - Since the planning horizon is infinite, the specific time step doesn't become as crucial. The agent is always concerned with maximizing the total discounted reward across all future time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability distributions refer to mathematical functions that describe the likelihood of different outcomes or events in a given random process. These distributions assign probabilities to all possible outcomes, with the probabilities summing up to 1. By using probability distributions, we can quantify uncertainty and model randomness in various phenomena, making them essential in statistics, machine learning, and other fields where uncertainty plays a role.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### `The RL Problem`\n",
    "\n",
    "- Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes expected return when the agent acts according to it.\n",
    "\n",
    "    - To talk about expected return, we first have to talk about probability distributions over trajectories.\n",
    "\n",
    "    - Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a T -step trajectory is:\n",
    "\n",
    "        $$P(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t)$$\n",
    "\n",
    "     - The expected return (for whichever measure), denoted by J(\\pi), is then:\n",
    "       \n",
    "       \n",
    "        $$J(\\pi) = \\int_{\\tau} P(\\tau|\\pi) R(\\tau)  = \\mathbb{E}_{\\tau\\sim \\pi} \\ R(\\tau)$$\n",
    "\n",
    "    - The central optimization problem in RL can then be expressed by\n",
    "\n",
    "        $$\\pi^* = \\arg \\max_{\\pi} J(\\pi)$$\n",
    "\n",
    "    - with \\pi^* being the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Functions:\n",
    "\n",
    "-  It’s often useful to know the value of a state, or state-action pair. By value, we mean the expected return if you start in that state or state-action pair, and then act according to a particular policy forever after. Value functions are used, one way or another, in almost every RL algorithm.\n",
    "\n",
    "    - **There are four main functions of note here.**\n",
    "\n",
    "    - The On-Policy Value Function, V^{\\pi}(s), which gives the expected return if you start in state s and always act according to policy \\pi:\n",
    "\n",
    "        $$V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s\\right.}$$\n",
    "\n",
    "    - The On-Policy Action-Value Function, Q^{\\pi}(s,a), which gives the expected return if you start in state s, take an arbitrary action a (which may not have come from the policy), and then forever after act according to policy \\pi:\n",
    "\n",
    "        $$Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}$$\n",
    "\n",
    "    - The Optimal Value Function, V^*(s), which gives the expected return if you start in state s and always act according to the optimal policy in the environment:\n",
    "\n",
    "        $$V^*(s) = \\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s\\right.}$$\n",
    "\n",
    "    - The Optimal Action-Value Function, Q^*(s,a), which gives the expected return if you start in state s, take an arbitrary action a, and then forever after act according to the optimal policy in the environment:\n",
    "\n",
    "        $$Q^*(s,a) = \\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi}{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Optimal Q-Function and the Optimal Action\n",
    "- There is an important connection between the optimal action-value function Q^*(s,a) and the action selected by the optimal policy. By definition, Q^*(s,a) gives the expected return for starting in state s, taking (arbitrary) action a, and then acting according to the optimal policy forever after.\n",
    "\n",
    "- The optimal policy in s will select whichever action maximizes the expected return from starting in s. As a result, if we have Q^*, we can directly obtain the optimal action, a^*(s), via\n",
    "\n",
    "    $$a^*(s) = \\arg \\max_a Q^* (s,a)$$\n",
    "\n",
    "Note: there may be multiple actions which maximize Q^*(s,a), in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bellman Equations\n",
    "\n",
    "- All four of the value functions obey special self-consistency equations called Bellman equations. The basic idea behind the Bellman equations is this:\n",
    "\n",
    "- The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n",
    "\n",
    "- The Bellman equations for the on-policy value functions are\n",
    "\n",
    "    $$V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi \\\\ s'\\sim P}{r(s,a) + \\gamma V^{\\pi}(s')}$$\n",
    "    $$Q^{\\pi}(s,a) = \\mathbb{E}_{s'\\sim P}[{r(s,a) + \\gamma \\mathbb{E}_{a'\\sim \\pi}{Q^{\\pi}(s',a')}}]$$\n",
    "\n",
    "    - where s' \\sim P is shorthand for s' \\sim P(\\cdot |s,a), indicating that the next state s' is sampled from the environment’s transition rules; a \\sim \\pi is shorthand for a \\sim \\pi(\\cdot|s); and a' \\sim \\pi is shorthand for a' \\sim \\pi(\\cdot|s').\n",
    "\n",
    "    The Bellman equations for the `optimal value functions` are\n",
    "\n",
    "\n",
    "    $$V^*(s) = \\max_a \\mathbb{E}_{s'\\sim P}[{r(s,a) + \\gamma V^*(s')}]$$\n",
    "    $$Q^*(s,a) = \\mathbb{E}_{s'\\sim P}[{r(s,a) + \\gamma \\max_{a'} Q^*(s',a')}]$$\n",
    "\n",
    "\n",
    "The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions, is the absence or presence of the \\max over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantage Functions\n",
    "- Sometimes in RL, we don’t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative advantage of that action. We make this concept precise with the advantage function.\n",
    "\n",
    "- The advantage function A^{\\pi}(s,a) corresponding to a policy \\pi describes how much better it is to take a specific action a in state s, over randomly selecting an action according to \\pi(\\cdot|s), assuming you act according to \\pi forever after. Mathematically, the advantage function is defined by\n",
    "\n",
    "    $$A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![C:\\Users\\razaa\\Desktop\\RLexp]('C:\\Users\\razaa\\Desktop\\RLexp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
